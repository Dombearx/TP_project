{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wczytywanie danych\n",
    "\n",
    "# dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "training_set = datasets.load_files(\"training_set\", encoding=\"utf-8\", random_state=420) \n",
    "# random_state miesza dane, wartość to seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'niedbal wyspa ogorzały ręka trzymać lejka i odwracać twarz wieźć przez kobieta odpowiadać wesoły zapytanie i przycinek czas męski śmiech swój łączyć ezyt chór cienki piskliwy śmiech dziewczę'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# przydatne atrybuty\n",
    "\n",
    "# training_set.filenames\n",
    "# training_set.data\n",
    "# training_set.target_names\n",
    "# training_set.target\n",
    "# training_set.target_names[training_set.target[3]]\n",
    "training_set.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1094, 3583)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizacja\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "train_count_vectors = count_vect.fit_transform(training_set.data)\n",
    "train_count_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vect.vocabulary_.get('koń') # liczba wystąpień słowa w całym zbiorze\n",
    "# print(train_count_vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf–idf - “Term Frequency times Inverse Document Frequency”\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "#fit our estimator to the data and transform our count-matrix to a tf-idf representation\n",
    "train_tfidf = tfidf_transformer.fit_transform(train_count_vectors) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB().fit(train_tfidf, training_set.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dlaczego z gorącego snu pierwszej młodości obudziła się nie tylko samotna i smutna, ale zarazem obrażona i z niewyschłą dotąd kroplą goryczy w sercu?' => P\n",
      "'Pod złotawym, a potem już siwiejącym wąsem zawsze purpurowe, zmysłowe jego usta układały się w wyraz lubości, ilekroć zobaczył jakąkolwiek ładną twarzyczkę lub zgrabną kibić niewieścią.' => P\n",
      "'Przed sobą, o kroków kilkanaście, zobaczyła wznoszącą się nad zbożem, rozłożystą i całą w słońcu stojącą gruszę polną; pień, gałęzie i wszystkie liście tego drzewa były złote' => P\n",
      "'Był to wraz z brzegiem rzeki zginający się nieco w półkole sznur siedlisk ludzkich, większych i mniejszych, wychylających ciemne swe profile z większych i mniejszych ogrodów.' => P\n"
     ]
    }
   ],
   "source": [
    "# takie sobie testy zdań\n",
    "test_data = ['Dlaczego z gorącego snu pierwszej młodości obudziła się nie tylko samotna i smutna, ale zarazem obrażona i z niewyschłą dotąd kroplą goryczy w sercu?', \n",
    "             'Pod złotawym, a potem już siwiejącym wąsem zawsze purpurowe, zmysłowe jego usta układały się w wyraz lubości, ilekroć zobaczył jakąkolwiek ładną twarzyczkę lub zgrabną kibić niewieścią.',\n",
    "            'Przed sobą, o kroków kilkanaście, zobaczyła wznoszącą się nad zbożem, rozłożystą i całą w słońcu stojącą gruszę polną; pień, gałęzie i wszystkie liście tego drzewa były złote',\n",
    "            'Był to wraz z brzegiem rzeki zginający się nieco w półkole sznur siedlisk ludzkich, większych i mniejszych, wychylających ciemne swe profile z większych i mniejszych ogrodów.']\n",
    "X_new_counts = count_vect.transform(test_data)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = classifier.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(test_data, predicted):\n",
    "     print('%r => %s' % (doc, training_set.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8226691042047533\n"
     ]
    }
   ],
   "source": [
    "# Tworzenie Naive Bayes Clf w pipeline i trenowanie\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "training_set = datasets.load_files(\"training_set\", encoding=\"utf-8\", random_state=420) \n",
    "clf.fit(training_set.data, training_set.target)\n",
    "\n",
    "# NotFittedError: CountVectorizer - Vocabulary wasn't fitted. - nwm co jest nie tak\n",
    "test_set = training_set.data\n",
    "predicted = clf.predict(test_set)\n",
    "print(np.mean(predicted == training_set.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# test_set = training_set.data\n",
    "# X_new_counts = count_vect.transform(test_set)\n",
    "# X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "# predicted = classifier.predict(X_new_tfidf)\n",
    "# print(np.mean(predicted == training_set.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.00      0.00      0.00        34\n",
      "           O       1.00      0.05      0.10       169\n",
      "           P       0.82      1.00      0.90       891\n",
      "\n",
      "    accuracy                           0.82      1094\n",
      "   macro avg       0.61      0.35      0.33      1094\n",
      "weighted avg       0.82      0.82      0.75      1094\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\domin\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(training_set.target, predicted, target_names=training_set.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9515539305301646\n"
     ]
    }
   ],
   "source": [
    "# # Tworzenie SVM Clf w pipeline i trenowanie\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=5, tol=None)),\n",
    "])\n",
    "\n",
    "training_set = datasets.load_files(\"training_set\", encoding=\"utf-8\", random_state=420)\n",
    "svm_clf.fit(training_set.data, training_set.target)\n",
    "\n",
    "test_set = training_set.data\n",
    "predicted = svm_clf.predict(test_set)\n",
    "print(np.mean(predicted == training_set.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      0.79      0.89        34\n",
      "           O       1.00      0.73      0.84       169\n",
      "           P       0.94      1.00      0.97       891\n",
      "\n",
      "    accuracy                           0.95      1094\n",
      "   macro avg       0.98      0.84      0.90      1094\n",
      "weighted avg       0.95      0.95      0.95      1094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(training_set.target, predicted, target_names=training_set.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=5, tol=None)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search dls SVM\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# parameters = {\n",
    "#     # parametry CountVectorizer\n",
    "#     \"\"\"...CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', \n",
    "#     strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', \n",
    "#     ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, \n",
    "#     dtype=<class 'numpy.int64'>)\"\"\"\n",
    "#     'vect__decode_error': ('strict', 'ignore', 'replace'),\n",
    "#     'vect__ngram_range': [(1, 1), (1, 2), (2, 2)], # (1, 1), (1, 2), (2, 2), (2, 3), (3, 3) \n",
    "#     'vect__max_df': [1.0, 0.90, 0.80, 0.70], # 1.0, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70\n",
    "#     'vect__min_df': [1, 0.10, 0.20, 0.30] # 0.05, 0.10, 0.15, 0.20, 0.25, 0.30\n",
    "    \n",
    "#     # parametry TfidfTransformer\n",
    "#     \"\"\"...TfidfTransformer(*, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\"\"\"\n",
    "#     #można coś jeszcze dodać niby, ale to chyba takie szczegóły, że nie ma sensu\n",
    "#     'tfidf__use_idf': (True, False),\n",
    "    \n",
    "#     # parametry SGDClassifier (SVM)\n",
    "#     \"\"\"SGDClassifier(loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, \n",
    "#     tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, \n",
    "#     power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, \n",
    "#     average=False)\"\"\"\n",
    "#     \"\"\"'clf__loss': The possible options are ‘hinge’, ‘log’, ‘modified_huber’, ‘squared_hinge’, ‘perceptron’, or a \n",
    "#     regression loss: ‘squared_loss’, ‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’.\n",
    "#     Trochę dużo tego \"\"\"\n",
    "#     'clf__alpha': (1e-2, 1e-3, 1e-4),\n",
    "#     'clf__shuffle': (True, False),\n",
    "#     'clf__class_weight': (None, 'balanced')     \n",
    "# }\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'vect__decode_error': ('strict', 'ignore', 'replace'), #'strict'\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2), (1, 3), (2, 3), (3, 3)], # (1, 1), (1, 2), (2, 2), (2, 3), (3, 3) \n",
    "    'vect__max_df': [1.0, 0.95, 0.90, 0.85], # 1.0, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70\n",
    "    'vect__min_df': [1, 0], # 0.05, 0.10, 0.15, 0.20, 0.25, 0.30 \n",
    "    'vect__binary': (False,True),\n",
    "    'tfidf__use_idf': (True,), # (True, False)\n",
    "    # można dodać jeszcze kilka parametrów tfidf niby, ale to chyba takie szczegóły, że nie ma sensu\n",
    "    'clf__alpha': (1e-2, 1e-3, 1e-4),\n",
    "    'clf__shuffle': (True,), # (True, False)\n",
    "    'clf__class_weight': (None, 'balanced'),\n",
    "}\n",
    "\n",
    "grid_search_clf = GridSearchCV(svm_clf, parameters, cv=5, n_jobs=3) # !!! n_jobs - LICZBA UŻYWANYCH RDZENI !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_clf = grid_search_clf.fit(training_set.data, training_set.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8537807381341377\n",
      "clf__alpha: 0.001\n",
      "clf__class_weight: 'balanced'\n",
      "clf__shuffle: True\n",
      "tfidf__use_idf: True\n",
      "vect__binary: True\n",
      "vect__decode_error: 'strict'\n",
      "vect__max_df: 1.0\n",
      "vect__min_df: 1\n",
      "vect__ngram_range: (1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n0.8510054844606947\\nclf__alpha: 0.0001\\nclf__class_weight: None\\nclf__shuffle: True\\ntfidf__use_idf: True\\nvect__decode_error: 'strict'\\nvect__max_df: 1.0\\nvect__ngram_range: (1, 2)\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(grid_search_clf.best_score_)\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, grid_search_clf.best_params_[param_name]))\n",
    "    \n",
    "\"\"\"\n",
    "0.8510054844606947\n",
    "clf__alpha: 0.0001\n",
    "clf__class_weight: None\n",
    "clf__shuffle: True\n",
    "tfidf__use_idf: True\n",
    "vect__decode_error: 'strict'\n",
    "vect__max_df: 1.0\n",
    "vect__ngram_range: (1, 2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8510054844606947\n",
      "clf__alpha: 0.0001\n",
      "clf__class_weight: None\n",
      "clf__shuffle: True\n",
      "tfidf__use_idf: True\n",
      "vect__decode_error: 'strict'\n",
      "vect__max_df: 1.0\n",
      "vect__min_df: 1\n",
      "vect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "print(grid_search_clf.best_score_)\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, grid_search_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=0.0001, class_weight=None random_state=42,\n",
    "                          max_iter=5, tol=None)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takie sobie testy zdań\n",
    "test_data = ['Dlaczego z gorącego snu pierwszej młodości obudziła się nie tylko samotna i smutna, ale zarazem obrażona i z niewyschłą dotąd kroplą goryczy w sercu?', \n",
    "             'Pod złotawym, a potem już siwiejącym wąsem zawsze purpurowe, zmysłowe jego usta układały się w wyraz lubości, ilekroć zobaczył jakąkolwiek ładną twarzyczkę lub zgrabną kibić niewieścią.',\n",
    "            'Przed sobą, o kroków kilkanaście, zobaczyła wznoszącą się nad zbożem, rozłożystą i całą w słońcu stojącą gruszę polną; pień, gałęzie i wszystkie liście tego drzewa były złote',\n",
    "            'Był to wraz z brzegiem rzeki zginający się nieco w półkole sznur siedlisk ludzkich, większych i mniejszych, wychylających ciemne swe profile z większych i mniejszych ogrodów.']\n",
    "X_new_counts = count_vect.transform(test_data)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = svm_pipeline.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(test_data, predicted):\n",
    "     print('%r => %s' % (doc, training_set.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SVC(kernel=\"rbf\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'clf__gamma':[0.1, 1, 10], 'clf__C':[1, 10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(svm_pipeline, param_grid=parameters, scoring=\"accuracy\", verbose=2, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] clf__C=1, clf__gamma=0.1 ........................................\n",
      "[CV] ......................... clf__C=1, clf__gamma=0.1, total=   0.1s\n",
      "[CV] clf__C=1, clf__gamma=0.1 ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......................... clf__C=1, clf__gamma=0.1, total=   0.1s\n",
      "[CV] clf__C=1, clf__gamma=0.1 ........................................\n",
      "[CV] ......................... clf__C=1, clf__gamma=0.1, total=   0.1s\n",
      "[CV] clf__C=1, clf__gamma=1 ..........................................\n",
      "[CV] ........................... clf__C=1, clf__gamma=1, total=   0.1s\n",
      "[CV] clf__C=1, clf__gamma=1 ..........................................\n",
      "[CV] ........................... clf__C=1, clf__gamma=1, total=   0.1s\n",
      "[CV] clf__C=1, clf__gamma=1 ..........................................\n",
      "[CV] ........................... clf__C=1, clf__gamma=1, total=   0.1s\n",
      "[CV] clf__C=1, clf__gamma=10 .........................................\n",
      "[CV] .......................... clf__C=1, clf__gamma=10, total=   0.1s\n",
      "[CV] clf__C=1, clf__gamma=10 .........................................\n",
      "[CV] .......................... clf__C=1, clf__gamma=10, total=   0.1s\n",
      "[CV] clf__C=1, clf__gamma=10 .........................................\n",
      "[CV] .......................... clf__C=1, clf__gamma=10, total=   0.1s\n",
      "[CV] clf__C=10, clf__gamma=0.1 .......................................\n",
      "[CV] ........................ clf__C=10, clf__gamma=0.1, total=   0.1s\n",
      "[CV] clf__C=10, clf__gamma=0.1 .......................................\n",
      "[CV] ........................ clf__C=10, clf__gamma=0.1, total=   0.1s\n",
      "[CV] clf__C=10, clf__gamma=0.1 .......................................\n",
      "[CV] ........................ clf__C=10, clf__gamma=0.1, total=   0.1s\n",
      "[CV] clf__C=10, clf__gamma=1 .........................................\n",
      "[CV] .......................... clf__C=10, clf__gamma=1, total=   0.1s\n",
      "[CV] clf__C=10, clf__gamma=1 .........................................\n",
      "[CV] .......................... clf__C=10, clf__gamma=1, total=   0.1s\n",
      "[CV] clf__C=10, clf__gamma=1 .........................................\n",
      "[CV] .......................... clf__C=10, clf__gamma=1, total=   0.1s\n",
      "[CV] clf__C=10, clf__gamma=10 ........................................\n",
      "[CV] ......................... clf__C=10, clf__gamma=10, total=   0.1s\n",
      "[CV] clf__C=10, clf__gamma=10 ........................................\n",
      "[CV] ......................... clf__C=10, clf__gamma=10, total=   0.1s\n",
      "[CV] clf__C=10, clf__gamma=10 ........................................\n",
      "[CV] ......................... clf__C=10, clf__gamma=10, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:    2.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(?u)...\n",
       "                                            coef0=0.0,\n",
       "                                            decision_function_shape='ovr',\n",
       "                                            degree=3, gamma='scale',\n",
       "                                            kernel='rbf', max_iter=-1,\n",
       "                                            probability=False,\n",
       "                                            random_state=None, shrinking=True,\n",
       "                                            tol=0.001, verbose=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'clf__C': [1, 10], 'clf__gamma': [0.1, 1, 10]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(training_set.data, training_set.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8537807381341377"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 10, 'clf__gamma': 0.1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SVC(kernel=\"rbf\", gamma = 0.1, C = 10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 SVC(C=10, break_ties=False, cache_size=200, class_weight=None,\n",
       "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
       "                     gamma=0.1, kernel='rbf', max_iter=-1, probability=False,\n",
       "                     random_state=None, shrinking=True, tol=0.001,\n",
       "                     verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_pipeline.fit(training_set.data, training_set.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-c0875eced336>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mX_new_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_new_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_new_tfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\domin\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\domin\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\domin\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1270\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1271\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1272\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\domin\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1129\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\domin\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\domin\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \"\"\"\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\domin\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    689\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 691\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "# takie sobie testy zdań\n",
    "test_data = ['Dlaczego z gorącego snu pierwszej młodości obudziła się nie tylko samotna i smutna, ale zarazem obrażona i z niewyschłą dotąd kroplą goryczy w sercu?', \n",
    "             'Pod złotawym, a potem już siwiejącym wąsem zawsze purpurowe, zmysłowe jego usta układały się w wyraz lubości, ilekroć zobaczył jakąkolwiek ładną twarzyczkę lub zgrabną kibić niewieścią.',\n",
    "            'Przed sobą, o kroków kilkanaście, zobaczyła wznoszącą się nad zbożem, rozłożystą i całą w słońcu stojącą gruszę polną; pień, gałęzie i wszystkie liście tego drzewa były złote',\n",
    "            'Był to wraz z brzegiem rzeki zginający się nieco w półkole sznur siedlisk ludzkich, większych i mniejszych, wychylających ciemne swe profile z większych i mniejszych ogrodów.']\n",
    "X_new_counts = count_vect.transform(test_data)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = svm_pipeline.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(test_data, predicted):\n",
    "     print('%r => %s' % (doc, training_set.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
