{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wczytywanie danych\n",
    "\n",
    "# dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "training_set = datasets.load_files(\"training_set\", encoding=\"utf-8\", random_state=420) \n",
    "# random_state miesza dane, wartość to seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'niedbal wyspa ogorzały ręka trzymać lejka i odwracać twarz wieźć przez kobieta odpowiadać wesoły zapytanie i przycinek czas męski śmiech swój łączyć ezyt chór cienki piskliwy śmiech dziewczę'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# przydatne atrybuty\n",
    "\n",
    "# training_set.filenames\n",
    "# training_set.data\n",
    "# training_set.target_names\n",
    "# training_set.target\n",
    "# training_set.target_names[training_set.target[3]]\n",
    "training_set.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1094, 3583)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizacja\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "train_count_vectors = count_vect.fit_transform(training_set.data)\n",
    "train_count_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3061)\t1\n",
      "  (0, 494)\t1\n",
      "  (0, 2376)\t1\n",
      "  (0, 2156)\t1\n",
      "  (0, 2487)\t1\n",
      "  (0, 3422)\t1\n",
      "  (0, 1186)\t1\n",
      "  (0, 2378)\t1\n",
      "  (0, 3525)\t1\n",
      "  (0, 1673)\t1\n",
      "  (0, 3180)\t1\n",
      "  (0, 2591)\t1\n",
      "  (0, 714)\t1\n",
      "  (0, 2806)\t1\n",
      "  (0, 349)\t1\n",
      "  (0, 2776)\t1\n",
      "  (0, 1170)\t1\n",
      "  (0, 2738)\t1\n"
     ]
    }
   ],
   "source": [
    "# count_vect.vocabulary_.get('koń') # liczba wystąpień słowa w całym zbiorze\n",
    "# print(train_count_vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3583 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 25 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf–idf - “Term Frequency times Inverse Document Frequency”\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "#fit our estimator to the data and transform our count-matrix to a tf-idf representation\n",
    "train_tfidf = tfidf_transformer.fit_transform(train_count_vectors) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB().fit(train_tfidf, training_set.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dlaczego z gorącego snu pierwszej młodości obudziła się nie tylko samotna i smutna, ale zarazem obrażona i z niewyschłą dotąd kroplą goryczy w sercu?' => P\n",
      "'Pod złotawym, a potem już siwiejącym wąsem zawsze purpurowe, zmysłowe jego usta układały się w wyraz lubości, ilekroć zobaczył jakąkolwiek ładną twarzyczkę lub zgrabną kibić niewieścią.' => P\n",
      "'Przed sobą, o kroków kilkanaście, zobaczyła wznoszącą się nad zbożem, rozłożystą i całą w słońcu stojącą gruszę polną; pień, gałęzie i wszystkie liście tego drzewa były złote' => P\n",
      "'Był to wraz z brzegiem rzeki zginający się nieco w półkole sznur siedlisk ludzkich, większych i mniejszych, wychylających ciemne swe profile z większych i mniejszych ogrodów.' => P\n"
     ]
    }
   ],
   "source": [
    "# takie sobie testy zdań\n",
    "test_data = ['Dlaczego z gorącego snu pierwszej młodości obudziła się nie tylko samotna i smutna, ale zarazem obrażona i z niewyschłą dotąd kroplą goryczy w sercu?', \n",
    "             'Pod złotawym, a potem już siwiejącym wąsem zawsze purpurowe, zmysłowe jego usta układały się w wyraz lubości, ilekroć zobaczył jakąkolwiek ładną twarzyczkę lub zgrabną kibić niewieścią.',\n",
    "            'Przed sobą, o kroków kilkanaście, zobaczyła wznoszącą się nad zbożem, rozłożystą i całą w słońcu stojącą gruszę polną; pień, gałęzie i wszystkie liście tego drzewa były złote',\n",
    "            'Był to wraz z brzegiem rzeki zginający się nieco w półkole sznur siedlisk ludzkich, większych i mniejszych, wychylających ciemne swe profile z większych i mniejszych ogrodów.']\n",
    "X_new_counts = count_vect.transform(test_data)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = classifier.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(test_data, predicted):\n",
    "     print('%r => %s' % (doc, training_set.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8226691042047533\n"
     ]
    }
   ],
   "source": [
    "# Tworzenie Naive Bayes Clf w pipeline i trenowanie\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "training_set = datasets.load_files(\"training_set\", encoding=\"utf-8\", random_state=420) \n",
    "clf.fit(training_set.data, training_set.target)\n",
    "\n",
    "# NotFittedError: CountVectorizer - Vocabulary wasn't fitted. - nwm co jest nie tak\n",
    "test_set = training_set.data\n",
    "predicted = clf.predict(test_set)\n",
    "print(np.mean(predicted == training_set.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8226691042047533\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# test_set = training_set.data\n",
    "# X_new_counts = count_vect.transform(test_set)\n",
    "# X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "# predicted = classifier.predict(X_new_tfidf)\n",
    "# print(np.mean(predicted == training_set.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.00      0.00      0.00        34\n",
      "           O       1.00      0.05      0.10       169\n",
      "           P       0.82      1.00      0.90       891\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      1094\n",
      "   macro avg       0.61      0.35      0.33      1094\n",
      "weighted avg       0.82      0.82      0.75      1094\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(training_set.target, predicted, target_names=training_set.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9570383912248629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mike\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# # Tworzenie SVM Clf w pipeline i trenowanie\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=5, tol=None)),\n",
    "])\n",
    "\n",
    "svm_clf.fit(training_set.data, training_set.target)\n",
    "\n",
    "test_set = training_set.data\n",
    "predicted = svm_clf.predict(test_set)\n",
    "print(np.mean(predicted == training_set.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      0.82      0.90        34\n",
      "           O       1.00      0.76      0.86       169\n",
      "           P       0.95      1.00      0.97       891\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1094\n",
      "   macro avg       0.98      0.86      0.91      1094\n",
      "weighted avg       0.96      0.96      0.95      1094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(training_set.target, predicted, target_names=training_set.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search dls SVM\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    # parametry CountVectorizer\n",
    "    \"\"\"...CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', \n",
    "    strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', \n",
    "    ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, \n",
    "    dtype=<class 'numpy.int64'>)\"\"\"\n",
    "    'vect__decode_error': ['strict', 'ignore', 'replace'],\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2), (2, 3), (3, 3)], # pewnie trzeba będzie zmniejszyć liczbę opcji\n",
    "    'vect__max_df': [1.0, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70], # pewnie trzeba będzie zmniejszyć liczbę opcji\n",
    "    'vect__min_df': [0.05, 0.10, 0.15, 0.20, 0.25, 0.30] # pewnie trzeba będzie zmniejszyć liczbę opcji\n",
    "    # parametry TfidfTransformer\n",
    "    \"\"\"...TfidfTransformer(*, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\"\"\"\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    # parametry SGDClassifier (SVM)\n",
    "    \"\"\"SGDClassifier(loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, \n",
    "    tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, \n",
    "    power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, \n",
    "    average=False)\"\"\"\n",
    "    'clf__alpha': (1e-2, 1e-3),\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
